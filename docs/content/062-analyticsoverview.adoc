[[analytics-overview]]
== Analytics

=== Overview

Analytics embody algorithms tailored to geospatial data.  Most analytics leverage Hadoop MapReduce for bulk computation.
Results of analytic jobs consist of vector or raster data stored in GeoWave.  The analytics infrastructure provides tools to
build algorithms in Spark.  For example, a Kryo serializer/deserializer enables exchange of SimpleFeatures and the GeoWaveInputFormat
supplies data to the Hadoop RDD <1>.

[NOTE]
<1> GeoWaveInputFormat does not remove duplicate features that reference polygons spanning multiple index regions.

The following algorithms are provided.


[width="80%",cols="2,10",options="header"]
|=========================================================
|Name |Description
|KMeans++|
A K-Means implementation to find K centroids over the population of data.
A set of preliminary sampling iterations find an optimal value of K and the an initial set of K centroids.
The algorithm produces K centroids and their associated polygons.  Each polygon represents the concave hull
containing all features associated with a centroid.
The algorithm supports drilling down multiple levels. At each level, the set centroids are determined
from the set of features associated the same centroid from the previous level.
|KMeans Jump|
Uses KMeans++ over a range of k, choosing an optimal k using an information theoretic based measurement.
|KMeans Parallel|
Performs a KMeans Parallel Cluster
|DBScan|
The Density Based Scanner algorithm produces a set of convex polygons for each region meeting density criteria.
Density of region is measured by a minimum cardinality of enclosed features within a specified distance from each other.
|Nearest Neighbors|
A infrastructure component that produces all the neighbors of a feature within a specific distance.
|=========================================================

=== Building

First build the main project, specifying the dependency versions.

[source, bash]
----
export BUILD_ARGS="-Daccumulo.version=1.6.0-cdh5.1.4 -Dhadoop.version=2.6.0-cdh5.4.0 -Dgeotools.version=13.0 -Dgeoserver.version=2.7.0 -Dvendor.version=cdh5 -Daccumulo.api=1.6 -P cloudera"
git clone https://github.com/ngageoint/geowave.git
cd geowave
mvn install -Dfindbugs.skip=true -Dformatter.skip=true -DskipITs=true -DskipTests=true $BUILD_ARGS
----

Next, build the analytics tool framework.

[source, bash]
----
cd analytics/mapreduce
mvn package -P analytics-singlejar -Dfindbugs.skip=true -Dformatter.skip=true -DskipITs=true -DskipTests=true $BUILD_ARGS
----

=== Running

The 'singlejar' jar file is located in the analytics/mapreduce/target/munged.   The jar is executed by Yarn.  Note, this file might not have the accumulo dependencies you need
to execute analytics.  In that case, you can use the normal tools jar.

The following assumes you have created a store named 'my_store' with accumulo connection parameters.

[source, bash]
----
yarn jar geowave-analytic-mapreduce-0.9.1-analytics-singlejar.jar analytic dbscan -cms 10 -emn 2 -emx 6 -pmd 1000 -orc 4 -hdfsbase /user/rwgdrummer --query.adapters gpxpoint my_store
----

The above command will execute DBSCAN.  The max distance between points is 10 meters, min hdfs input split is 2, max hdfs input split is 6, max search distance is 1000 meters, 
reducer count is 4, the temporary files needed by this job are stored in /user/rwgdrummer, and the data executed against DBSCAN is 'gpxpoint'.  The accumulo connection
parameters are loaded from my_store.